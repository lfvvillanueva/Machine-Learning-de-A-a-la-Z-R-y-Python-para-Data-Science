{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Máquinas de Vectores de Soporte (SVM): Una Guía Completa\n",
    "\n",
    "### Explicación Sencilla de SVM\n",
    "\n",
    "Las Máquinas de Vectores de Soporte (SVM) son un tipo de modelo de aprendizaje supervisado utilizado tanto para clasificación como para regresión, aunque son más comúnmente usadas para tareas de clasificación. SVM busca encontrar un hiperplano que mejor divida un conjunto de datos en dos clases, maximizando la distancia entre los puntos de datos más cercanos al hiperplano (vectores de soporte) y el hiperplano mismo. Esto ayuda al modelo a generalizar mejor a nuevos datos.\n",
    "\n",
    "### Explicación Detallada y Matemáticas de SVM\n",
    "\n",
    "#### Formulación Matemática\n",
    "\n",
    "1. **Hiperplano**:\n",
    "   Un hiperplano en un espacio $n$-dimensional es definido por la ecuación:\n",
    "   $$\n",
    "   \\mathbf{w} \\cdot \\mathbf{x} - b = 0\n",
    "   $$\n",
    "   donde $\\mathbf{w}$ es el vector normal al hiperplano, $\\mathbf{x}$ son los puntos de datos, y $b$ es el sesgo.\n",
    "\n",
    "2. **Margen Máximo**:\n",
    "   SVM busca maximizar el margen, que es la distancia que puedes mover el hiperplano sin tocar los datos. El margen se calcula como:\n",
    "   $$\n",
    "   \\frac{2}{\\|\\mathbf{w}\\|}\n",
    "   $$\n",
    "   Los datos deben clasificarse correctamente, lo que se impone por:\n",
    "   $$\n",
    "   y_i (\\mathbf{w} \\cdot \\mathbf{x}_i - b) \\geq 1, \\quad \\forall i\n",
    "   $$\n",
    "\n",
    "3. **Función de Costo y Lagrangianos**:\n",
    "   La optimización se realiza mediante la función lagrangiana:\n",
    "   $$\n",
    "   L(\\mathbf{w}, b, \\alpha) = \\frac{1}{2} \\|\\mathbf{w}\\|^2 - \\sum_{i=1}^{n} \\alpha_i [y_i (\\mathbf{w} \\cdot \\mathbf{x}_i - b) - 1]\n",
    "   $$\n",
    "   donde $\\alpha_i$ son los multiplicadores de Lagrange.\n",
    "\n",
    "4. **Kernel Trick**:\n",
    "   Para datos no linealmente separables, se utiliza el \"truco del kernel\" para mapear los datos a un espacio dimensional más alto donde sí es posible separarlos linealmente.\n",
    "\n",
    "#### Validación y Métricas\n",
    "\n",
    "- **Matriz de Confusión**: Incluye verdaderos positivos, verdaderos negativos, falsos positivos y falsos negativos.\n",
    "- **Precisión, Recall y F1-Score**: Métricas clave derivadas de la matriz de confusión.\n",
    "- **Validación Cruzada**: Usada para asegurar que el modelo generaliza bien a nuevos datos.\n",
    "- **Curva ROC y AUC**: Proporcionan una medida de la capacidad del modelo para discriminar entre clases.\n",
    "\n",
    "#### Comparación con Otros Modelos\n",
    "\n",
    "- **Regresión Logística**: Más propensa al sobreajuste pero más simple y fácil de implementar.\n",
    "- **Redes Neuronales**: Más flexibles y capaces de capturar relaciones complejas, aunque pueden requerir más datos y ser propensas al sobreajuste.\n",
    "- **Árboles de Decisión/Ensemble**: Generalmente más rápidos de entrenar y pueden manejar características categóricas y faltantes más fácilmente que SVM, pero pueden no ser tan efectivos en presencia de relaciones no lineales complejas entre características y etiquetas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión de Vectores de Soporte (SVR): Una Guía Completa\n",
    "\n",
    "### Explicación Sencilla de SVR\n",
    "\n",
    "La Regresión de Vectores de Soporte (SVR) es una variante del SVM utilizada para problemas de regresión. A diferencia de SVM, que se usa para clasificación, SVR se utiliza para predecir un valor continuo. El objetivo principal de SVR no es construir un hiperplano que separe los datos en clases, sino encontrar un hiperplano que se ajuste mejor a los datos dentro de un margen determinado, denominado $\\epsilon$-tubo, buscando minimizar los errores dentro de este margen mientras se ignora los errores que son menores que el margen.\n",
    "\n",
    "### Explicación Detallada y Matemáticas de SVR\n",
    "\n",
    "#### Formulación Matemática\n",
    "\n",
    "1. **Modelo y Hiperplano**:\n",
    "   La idea central de SVR es encontrar una función $f(x)$ que tenga a lo más un desvío $\\epsilon$ de los valores reales $y_i$ para todos los datos de entrenamiento, y al mismo tiempo sea tan plana como sea posible. La función generalmente se define como:\n",
    "   $$\n",
    "   f(x) = \\mathbf{w} \\cdot \\mathbf{x} + b\n",
    "   $$\n",
    "   donde $\\mathbf{w}$ es el vector de pesos y $b$ es el término de sesgo.\n",
    "\n",
    "2. **$\\epsilon$-insensitive Loss**:\n",
    "   SVR utiliza la función de pérdida $\\epsilon$-insensitive, que no penaliza errores menores que un cierto umbral $\\epsilon$. Esta se define como:\n",
    "   $$\n",
    "   |y - f(x)|_\\epsilon = \\max(0, |y - f(x)| - \\epsilon)\n",
    "   $$\n",
    "   donde $|y - f(x)| > \\epsilon$ implica un costo, mientras que los errores dentro del margen $\\epsilon$ no tienen penalización.\n",
    "\n",
    "3. **Formulación del Problema de Optimización**:\n",
    "   La optimización para SVR se formula como la minimización de:\n",
    "   $$\n",
    "   \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^{n} (\\xi_i + \\xi_i^*)\n",
    "   $$\n",
    "   sujeto a las restricciones:\n",
    "   $$\n",
    "   y_i - \\mathbf{w} \\cdot \\mathbf{x}_i - b \\leq \\epsilon + \\xi_i\n",
    "   $$\n",
    "   $$\n",
    "   \\mathbf{w} \\cdot \\mathbf{x}_i + b - y_i \\leq \\epsilon + \\xi_i^*\n",
    "   $$\n",
    "   $$\n",
    "   \\xi_i, \\xi_i^* \\geq 0\n",
    "   $$\n",
    "   donde $C$ es el parámetro de regularización que controla el compromiso entre el margen del tubo y el término de penalización de la función de coste, y $\\xi_i$ y $\\xi_i^*$ son las variables de holgura que miden el grado de violación del margen $\\epsilon$.\n",
    "\n",
    "#### Validación y Métricas\n",
    "\n",
    "Las métricas comunes para validar modelos SVR incluyen:\n",
    "\n",
    "- **MSE (Mean Squared Error)**: Promedio del cuadrado de los errores entre los valores predichos y reales.\n",
    "- **MAE (Mean Absolute Error)**: Promedio del valor absoluto de los errores.\n",
    "- **R-cuadrado**: Indica cuánta variación de los datos es explicada por el modelo.\n",
    "\n",
    "#### Comparación con Otros Modelos de Regresión\n",
    "\n",
    "- **Regresión Lineal**: SVR ofrece mayor flexibilidad para modelar no linealidades y es más robusto a los outliers.\n",
    "- **Árboles de Decisión y Modelos Ensemble**: Estos modelos son fáciles de interpretar y pueden ser más rápidos de entrenar, pero pueden ser más sensibles a los datos atípicos comparados con SVR.\n",
    "- **Redes Neuronales**: Capaces de modelar relaciones aún más complejas que SVR, pero requieren una selección cuidadosa de la arquitectura y más datos para un entrenamiento eficaz.\n",
    "\n",
    "SVR es especialmente útil en escenarios donde los datos no se ajustan bien a modelos lineales simples y donde la resistencia a outliers es crucial.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
